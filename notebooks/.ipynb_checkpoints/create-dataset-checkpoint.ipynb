{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 04:31:00.851521: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-23 04:31:01.132268: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Optional\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(obj):\n",
    "    return obj is not None\n",
    "\n",
    "\n",
    "def write_image(image_path: Union[str, Path], image: np.ndarray):\n",
    "    \"\"\"Writes an RGB image using OpenCV.\"\"\"\n",
    "    if isinstance(image_path, Path):\n",
    "        image_path = str(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(image_path, image)\n",
    "\n",
    "\n",
    "def read_image(image_path: Union[str, Path]) -> np.ndarray:\n",
    "    \"\"\"Reads an image from a path and converts it to RGB format.\"\"\"\n",
    "    if isinstance(image_path, Path):\n",
    "        image_path = str(image_path)\n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "\n",
    "def read_mask(mask_path: Union[str, Path]) -> np.ndarray:\n",
    "    \"\"\"Reads a mask from a path and transform it to binary.\"\"\"\n",
    "    if isinstance(mask_path, Path):\n",
    "        mask_path = str(mask_path)\n",
    "    mask = cv2.imread(mask_path, 0)\n",
    "    if mask.max() == 255:\n",
    "        mask = mask / 255.0\n",
    "    assert_mask_is_binary(mask)\n",
    "    return mask.astype(np.float32)\n",
    "\n",
    "\n",
    "def assert_mask_is_binary(mask: np.ndarray):\n",
    "    \"\"\"Counts all the pixels different to zero and one to check if binary.\"\"\"\n",
    "    assert (\n",
    "        np.count_nonzero((mask != 0) & (mask != 1)) == 0\n",
    "    ), f\"Mask is not binary. Unique values: {np.unique(mask)}\"\n",
    "\n",
    "\n",
    "def read_images_grid(\n",
    "    patches_paths: Union[List[str], List[Path]]\n",
    ") -> List[List[np.ndarray]]:\n",
    "    patches = []\n",
    "    for i, patches_paths_row in enumerate(patches_paths):\n",
    "        images_row = []\n",
    "        for j, patch_path in enumerate(patches_paths_row):\n",
    "            image = (\n",
    "                read_image(patch_path)\n",
    "                if exists(patch_path)\n",
    "                else np.zeros((1024, 1024, 3))\n",
    "            )\n",
    "            images_row.append(image)\n",
    "        patches.append(images_row)\n",
    "    return patches\n",
    "\n",
    "\n",
    "def crop_black_frames_from_image(img: np.ndarray) -> np.ndarray:\n",
    "    positions = np.nonzero(img)\n",
    "    top = positions[0].min()\n",
    "    bottom = positions[0].max()\n",
    "    left = positions[1].min()\n",
    "    right = positions[1].max()\n",
    "    return img[top:bottom, left:right]\n",
    "\n",
    "\n",
    "def sort_paths_from_idxs(\n",
    "    patches_paths: Union[List[str], List[Path]]\n",
    ") -> List[List[Union[str, Path]]]:\n",
    "    \"\"\"Creates a grid of paths from a list of paths based on their ids.\"\"\"\n",
    "    if isinstance(patches_paths[0], str):\n",
    "        patches_paths = [Path(patch_path) for patch_path in patches_paths]\n",
    "    idxs = [patch_path.stem.split(\"_\")[-2:] for patch_path in patches_paths]\n",
    "    idxs = [[int(i[0]), int(i[1])] for i in idxs]\n",
    "\n",
    "    # Create the grid.\n",
    "    (y_min, x_min), (y_max, x_max) = np.min(idxs, axis=0), np.max(idxs, axis=0)\n",
    "    sorted_paths, missing_patches_idxs = [], []\n",
    "    for j in range(y_max + 1):\n",
    "        row_paths = []\n",
    "        for i in range(x_max + 1):\n",
    "            if [j, i] not in idxs:\n",
    "                missing_patches_idxs.append([j, i])\n",
    "                row_paths.append(None)\n",
    "            else:\n",
    "                row_paths.append(patches_paths[idxs.index([j, i])])\n",
    "        sorted_paths.append(row_paths)\n",
    "\n",
    "    return sorted_paths, missing_patches_idxs\n",
    "\n",
    "\n",
    "def create_and_save_images_for_sample(\n",
    "    reconstructed_sample_image_path: Union[str, Path],\n",
    "    sample_patches_paths: List[Union[str, Path]],\n",
    "    reconstructed_sample_mask_path: Optional[Union[str, Path]] = None,\n",
    "    sample_mask_path: Optional[Union[str, Path]] = None,\n",
    "):\n",
    "    sorted_sample_patches_paths, _ = sort_paths_from_idxs(sample_patches_paths)\n",
    "    sorted_sample_patches = read_images_grid(sorted_sample_patches_paths)\n",
    "    reconstructed_sample_image = np.vstack(\n",
    "        [np.hstack(row) for row in sorted_sample_patches]\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Process the reconstructed image.\n",
    "    reconstructed_sample_image = crop_black_frames_from_image(\n",
    "        reconstructed_sample_image\n",
    "    )\n",
    "    reconstructed_sample_image = reconstructed_sample_image / 255.0\n",
    "    write_image(str(reconstructed_sample_image_path), reconstructed_sample_image)\n",
    "\n",
    "    # Read and resize mask to cropped reconstructed image.\n",
    "    if exists(sample_mask_path):\n",
    "        sample_mask = read_mask(sample_mask_path)\n",
    "        height, width = reconstructed_sample_image.shape[:2]\n",
    "        resized_sample_mask = cv2.resize(sample_mask, (width, height))\n",
    "        cv2.imwrite(str(reconstructed_sample_mask_path), resized_sample_mask)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples with masks: 33 \n",
      "\n",
      "Samples ids without masks:\n",
      "- TCGA-99-8028-01Z-00-DX1.23de89b1-67f8-41fb-980a-010ea190d687\n",
      "- TCGA-99-8032-01Z-00-DX1.7380b78f-ea25-43e0-ac90-194b5c6b1432\n",
      "- TCGA-J2-8194-01Z-00-DX1.7700924D-B6AF-46A7-A7D7-B5C17A66C5F7\n",
      "- TCGA-MP-A4TK-01Z-00-DX1.57494698-D9D9-4C04-AAB2-16616CCFDCC9\n",
      "- TCGA-NJ-A55R-01Z-00-DX1.2E2B3642-4E1C-47DB-AF7B-988D586C0986\n"
     ]
    }
   ],
   "source": [
    "dset_dir = Path(\"/data/histopathology/TCGA/\")\n",
    "\n",
    "patches_dir = dset_dir / \"patches\"\n",
    "masks_dir = dset_dir / \"masks\"\n",
    "\n",
    "# In the patches directory are more directories and in the masks directory are png files.\n",
    "patches_dirs = sorted(patches_dir.iterdir())\n",
    "masks_paths = sorted(masks_dir.iterdir())\n",
    "\n",
    "mask_sample_ids = [mask_path.stem[:-5] for mask_path in masks_paths]\n",
    "patches_sample_ids = [patch_dir.name for patch_dir in patches_dirs]\n",
    "test_sample_ids = sorted(list(set(patches_sample_ids).difference(mask_sample_ids)))\n",
    "train_sample_ids = sorted(list(set(patches_sample_ids).intersection(mask_sample_ids)))\n",
    "\n",
    "print(f\"Number of samples with masks: {len(train_sample_ids)} \\n\")\n",
    "print(f\"Samples ids without masks:\", *test_sample_ids, sep=\"\\n- \")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Fold: Train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e8c56c0f9a414bb0d3e5ac03963185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating training images for fold 0:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94813d5c33e04d64aa940930fd2ed78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val images for fold 0:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170f50417325400d8e69a450abb3836d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating training images for fold 1:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ef4328e1684eca88e7b66f5a8747a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val images for fold 1:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b42269443884ca4ac885620100fe7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating training images for fold 2:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fed04fbc4e34d09b0326efae448a418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val images for fold 2:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c77f28a711477f87555df8c9a4acdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating training images for fold 3:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945ff205f1004035afb67cc3a11d9fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val images for fold 3:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd049a612e5e4223a50f32bbd8048711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating training images for fold 4:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42c34068f4a43419be530e114181046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val images for fold 4:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_dir = Path(\"/data/histopathology/TCGA/reconstructed\")\n",
    "column_names = [\n",
    "    \"sample_id\",\n",
    "    \"source_patches_dir\",\n",
    "    \"source_mask_path\",\n",
    "    \"target_image_path\",\n",
    "    \"target_mask_path\",\n",
    "]\n",
    "\n",
    "# Do a KFold split to create a validation set.\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for i, (train_idxs, val_idxs) in enumerate(kf.split(train_sample_ids)):\n",
    "    kfold_train_sample_ids = np.array(train_sample_ids)[train_idxs]\n",
    "    kfold_val_sample_ids = np.array(train_sample_ids)[val_idxs]\n",
    "\n",
    "    fold_dir = target_dir / f\"fold{i}\"\n",
    "    train_dir = fold_dir / \"train\"\n",
    "    train_images_dir = train_dir / \"images\"\n",
    "    train_masks_dir = train_dir / \"masks\"\n",
    "    train_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_masks_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_fold_dict = {col: [] for col in column_names}\n",
    "    for sample_id in tqdm(\n",
    "        kfold_train_sample_ids, desc=f\"Creating training images for fold {i}\"\n",
    "    ):\n",
    "        # Read the sources patches and masks.\n",
    "        sample_patches_paths = sorted((patches_dir / sample_id).iterdir())\n",
    "        sample_mask_path = masks_dir / f\"{sample_id}_mask.png\"\n",
    "\n",
    "        # Create the reconstructed images and masks.\n",
    "        reconstructed_sample_image_path = (\n",
    "            train_images_dir / f\"{sample_id}_reconstructed.png\"\n",
    "        )\n",
    "        reconstructed_sample_mask_path = (\n",
    "            train_masks_dir / f\"{sample_id}_reconstructed.png\"\n",
    "        )\n",
    "        create_and_save_images_for_sample(\n",
    "            reconstructed_sample_image_path,\n",
    "            sample_patches_paths,\n",
    "            reconstructed_sample_mask_path,\n",
    "            sample_mask_path,\n",
    "        )\n",
    "\n",
    "        # Append to the fold dict.\n",
    "        train_fold_dict[\"sample_id\"].append(sample_id)\n",
    "        train_fold_dict[\"source_patches_dir\"].append(str(patches_dir / sample_id))\n",
    "        train_fold_dict[\"source_mask_path\"].append(str(sample_mask_path))\n",
    "        train_fold_dict[\"target_image_path\"].append(\n",
    "            str(reconstructed_sample_image_path)\n",
    "        )\n",
    "        train_fold_dict[\"target_mask_path\"].append(str(reconstructed_sample_mask_path))\n",
    "\n",
    "    # Save the fold dicts as csv files.\n",
    "    train_fold_df = pd.DataFrame(train_fold_dict)\n",
    "    train_fold_df.to_csv(fold_dir / \"train.csv\", index=False)\n",
    "\n",
    "    val_dir = fold_dir / \"val\"\n",
    "    val_images_dir = val_dir / \"images\"\n",
    "    val_masks_dir = val_dir / \"masks\"\n",
    "    val_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    val_masks_dir.mkdir(parents=True, exist_ok=True)\n",
    "    val_fold_dict = {col: [] for col in column_names}\n",
    "    for sample_id in tqdm(\n",
    "        kfold_val_sample_ids, desc=f\"Creating val images for fold {i}\"\n",
    "    ):\n",
    "        # Read the sources patches and masks.\n",
    "        sample_patches_paths = sorted((patches_dir / sample_id).iterdir())\n",
    "        sample_mask_path = masks_dir / f\"{sample_id}_mask.png\"\n",
    "\n",
    "        # Create the reconstructed images and masks.\n",
    "        reconstructed_sample_image_path = (\n",
    "            val_images_dir / f\"{sample_id}_reconstructed.png\"\n",
    "        )\n",
    "        reconstructed_sample_mask_path = (\n",
    "            val_masks_dir / f\"{sample_id}_reconstructed.png\"\n",
    "        )\n",
    "        create_and_save_images_for_sample(\n",
    "            reconstructed_sample_image_path,\n",
    "            sample_patches_paths,\n",
    "            reconstructed_sample_mask_path,\n",
    "            sample_mask_path,\n",
    "        )\n",
    "\n",
    "        # Append to the fold dict.\n",
    "        val_fold_dict[\"sample_id\"].append(sample_id)\n",
    "        val_fold_dict[\"source_patches_dir\"].append(str(patches_dir / sample_id))\n",
    "        val_fold_dict[\"source_mask_path\"].append(str(sample_mask_path))\n",
    "        val_fold_dict[\"target_image_path\"].append(str(reconstructed_sample_image_path))\n",
    "        val_fold_dict[\"target_mask_path\"].append(str(reconstructed_sample_mask_path))\n",
    "\n",
    "    # Save the fold dicts as csv files.\n",
    "    val_fold_df = pd.DataFrame(val_fold_dict)\n",
    "    val_fold_df.to_csv(fold_dir / \"val.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch extraction for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract patches from a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image, patch_size, stride):\n",
    "    patches = []\n",
    "    for i in range(0, image.shape[0] - patch_size + 1, stride):\n",
    "        for j in range(0, image.shape[1] - patch_size + 1, stride):\n",
    "            patch = image[i : i + patch_size, j : j + patch_size, :]\n",
    "            patches.append(patch)\n",
    "    return patches\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each fold, extract the patches from the reconstructed images and store them in a new \"patches\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85de2d22822849e7a93fc79c52fafee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting patches for fold 0:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m image \u001b[39m=\u001b[39m read_image(image_path)\n\u001b[1;32m     14\u001b[0m mask \u001b[39m=\u001b[39m read_image(masks_dir \u001b[39m/\u001b[39m image_path\u001b[39m.\u001b[39mname)\n\u001b[0;32m---> 15\u001b[0m image_mask_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mstack([image, mask])\n\u001b[1;32m     16\u001b[0m image_patches \u001b[39m=\u001b[39m extract_patches(image, \u001b[39m224\u001b[39m, \u001b[39m56\u001b[39m)\n\u001b[1;32m     17\u001b[0m mask_patches \u001b[39m=\u001b[39m extract_patches(mask, \u001b[39m224\u001b[39m, \u001b[39m56\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py:1484\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1482\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1483\u001b[0m     \u001b[39m# If the input is a constant list, it can be converted to a constant op\u001b[39;00m\n\u001b[0;32m-> 1484\u001b[0m     \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(values, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1485\u001b[0m   \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[1;32m   1486\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Input list contains non-constant tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1642\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1633\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1634\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1635\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1639\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1641\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1642\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1644\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1645\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py:344\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    343\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 344\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py:268\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    173\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    269\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py:280\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    279\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 280\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    282\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    283\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py:305\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    304\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    306\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    fold_dir = target_dir / f\"fold{i}\"\n",
    "    images_dir = fold_dir / \"train\" / \"images\"\n",
    "    masks_dir = fold_dir / \"train\" / \"masks\"\n",
    "    patches_dir = fold_dir / \"train\" / \"patches\"\n",
    "    image_patches_dir = patches_dir / \"images\"\n",
    "    mask_patches_dir = patches_dir / \"masks\"\n",
    "    image_patches_dir.mkdir(parents=True, exist_ok=True)\n",
    "    mask_patches_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sorted_images = sorted(images_dir.iterdir())\n",
    "    for image_path in tqdm(sorted_images, desc=f\"Extracting patches for fold {i}\"):\n",
    "        image = read_image(image_path)\n",
    "        mask = read_image(masks_dir / image_path.name)\n",
    "        image_mask_batch = tf.stack([image, mask])\n",
    "        image_patches = extract_patches(image, 224, 56)\n",
    "        mask_patches = extract_patches(mask, 224, 56)\n",
    "        for j, (image_patch, mask_patch) in enumerate(zip(image_patches, mask_patches)):\n",
    "            image_patch_path = image_patches_dir / f\"{image_path.stem}_{j}.png\"\n",
    "            mask_patch_path = mask_patches_dir / f\"{image_path.stem}_{j}.png\"\n",
    "            write_image(image_patch_path, image_patch)\n",
    "            cv2.imwrite(str(mask_patch_path), mask_patch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7904a9a72d6f4eb4a7cf64f31b80cd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating test images:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_dir = Path(\"/data/histopathology/TCGA/reconstructed\")\n",
    "column_names = [\n",
    "    \"sample_id\",\n",
    "    \"source_patches_dir\",\n",
    "    \"source_mask_path\",\n",
    "    \"target_image_path\",\n",
    "    \"target_mask_path\",\n",
    "]\n",
    "\n",
    "test_dir = target_dir / \"test\"\n",
    "test_images_dir = test_dir / \"images\"\n",
    "test_masks_dir = test_dir / \"masks\"\n",
    "test_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_masks_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_fold_dict = {col: [] for col in column_names}\n",
    "\n",
    "for sample_id in tqdm(test_sample_ids, desc=\"Creating test images\"):\n",
    "    # Read the sources patches and masks.\n",
    "    sample_patches_paths = sorted((patches_dir / sample_id).iterdir())\n",
    "    sample_mask_path = masks_dir / f\"{sample_id}_mask.png\"\n",
    "    assert not sample_mask_path.exists()\n",
    "\n",
    "    # Create the reconstructed images and masks.\n",
    "    reconstructed_sample_image_path = test_images_dir / f\"{sample_id}_reconstructed.png\"\n",
    "    create_and_save_images_for_sample(\n",
    "        reconstructed_sample_image_path,\n",
    "        sample_patches_paths,\n",
    "    )\n",
    "\n",
    "    # Append to the fold dict.\n",
    "    test_fold_dict[\"sample_id\"].append(sample_id)\n",
    "    test_fold_dict[\"source_patches_dir\"].append(str(patches_dir / sample_id))\n",
    "    test_fold_dict[\"source_mask_path\"].append(None)\n",
    "    test_fold_dict[\"target_image_path\"].append(str(reconstructed_sample_image_path))\n",
    "    test_fold_dict[\"target_mask_path\"].append(None)\n",
    "\n",
    "# Save the fold dicts as csv files.\n",
    "test_fold_df = pd.DataFrame(test_fold_dict)\n",
    "test_fold_df.to_csv(test_dir / \"test.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
